{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SvZIlK9jFkN0"
   },
   "source": [
    "# Analysis Code\n",
    "\n",
    "\n",
    "In this notebook we analyse the results from the experiments three ways. \n",
    "\n",
    "\n",
    "*   Gather per experiment performance metrics\n",
    "*   Create a mean prediction minus target dataset \n",
    "*   Aggregate the performance metrics across the dataset-algorithm combinations\n",
    "\n",
    "We analyse both the **test set** --- which is disjoint but created from the same dataset as the training set, and an **independent test set** --- which  produced separately \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FdWaaDkpV9kr"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error\n",
    "import scipy\n",
    "\n",
    "file_loc = '/content/drive/My Drive/Yeast_Growth_Project/predictions/'\n",
    "data_loc = '/content/drive/My Drive/Yeast_Growth_Project/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GdOnKLE7WNCp"
   },
   "outputs": [],
   "source": [
    "target = pd.read_csv(data_loc + 'test_growth_target.csv')\n",
    "target = target.iloc[1:, 1].to_numpy().reshape(227,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-k8XzoQFzaiF"
   },
   "source": [
    "# Gathering Error Statistics on the Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0TGJhK97yy2d"
   },
   "source": [
    "Here we gather the set of error statistics for a set of experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jp1I2_g2yzZ1"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_stats_for_each_run(file_name, data_1, data_2,  integration_type, method_type, number):\n",
    "  maes = np.zeros(number)\n",
    "  mdaes = np.zeros(number)\n",
    "  rmses = np.zeros(number)\n",
    "  pccs = np.zeros(number)\n",
    "  for x in range(number):\n",
    "    pred = pd.read_csv(file_loc + file_name + '_Predictions_' + str(x) + '.csv')\n",
    "    if pred.shape == (227, 2): # The R code models have a different format in predictions\n",
    "      pred = np.array(pred.iloc[:, 1]).reshape(227,1) \n",
    "    elif pred.shape == (228, 2):\n",
    "      pred = np.array(pred.iloc[1:, 1]).reshape(227,1) \n",
    "    else:\n",
    "      pred = np.array(pred).reshape(227,1)\n",
    "    maes[x] = mean_absolute_error(pred, target)\n",
    "    mdaes[x] = median_absolute_error(pred, target)\n",
    "    rmses[x] = np.sqrt(mean_squared_error(pred,target))\n",
    "    pccs[x] = scipy.stats.pearsonr(pred[:,0], target[:,0])[0]\n",
    "\n",
    "  return pd.DataFrame({'file_name' : [file_name] * number,\n",
    "                          'integration_type' : [integration_type] * number, \n",
    "                          'method_type' : [method_type] * number,\n",
    "                           'data_1' : [data_1] * number, \n",
    "                           'data_2' : [data_2] * number, \n",
    "                       'mae' : maes,\n",
    "                       'mdae' : mdaes,\n",
    "                       'rmse' : rmses,\n",
    "                       'pcc' : pccs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "47lsTINWyowc"
   },
   "source": [
    "Calculate the mean and confidence of a list of stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UaxuAONy_ZSY"
   },
   "outputs": [],
   "source": [
    "def mean_and_conf(stat):\n",
    "  return np.mean(stat), np.std(stat)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AcQczkIXy9o5"
   },
   "source": [
    "Summarises the set of results from an experiment as a single set of metrics (means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xn18-75Qy9Fa"
   },
   "outputs": [],
   "source": [
    "\n",
    "def summarise_results(file_name,  data_1, data_2, integration_type, method_type, number):\n",
    "  maes = np.zeros(number)\n",
    "  mdaes = np.zeros(number)\n",
    "  rmses = np.zeros(number)\n",
    "  pccs = np.zeros(number)\n",
    "  for x in range(number):\n",
    "    pred = pd.read_csv(file_loc + file_name + '_Predictions_' + str(x) + '.csv')\n",
    "    if pred.shape == (227, 2): # The R code models have a different format in predictions\n",
    "      pred = np.array(pred.iloc[:, 1]).reshape(227,1) \n",
    "    elif pred.shape == (228, 2):\n",
    "      pred = np.array(pred.iloc[1:, 1]).reshape(227,1)\n",
    "      \n",
    "    else:\n",
    "      pred = np.array(pred).reshape(227,1)\n",
    "    maes[x] = mean_absolute_error(pred, target)\n",
    "    mdaes[x] = median_absolute_error(pred, target)\n",
    "    rmses[x] = np.sqrt(mean_squared_error(pred,target))\n",
    "    pccs[x] = scipy.stats.pearsonr(pred[:,0], target[:,0])[0]\n",
    "    \n",
    "    mae_m, mae_std = mean_and_conf(maes)\n",
    "    mdae_m, mdae_std = mean_and_conf(mdaes)\n",
    "    rmse_m, rmse_std = mean_and_conf(rmses)\n",
    "    pcc_m, pcc_std = mean_and_conf(pccs)\n",
    "  return pd.DataFrame({'file_name' : file_name, \n",
    "                       'integration_type' : integration_type,\n",
    "                       'method_type' : method_type, \n",
    "                       'data_1' :data_1,\n",
    "                       'data_2' : data_2,\n",
    "                       'mae_mean' : mae_m,\n",
    "                       'mae_std' : mae_std,\n",
    "                       'mdae_mean' : mdae_m,\n",
    "                       'mdae_std' : mdae_std,\n",
    "                       'rmse_mean' : rmse_m,\n",
    "                       'rmse_std' : rmse_std,\n",
    "                       'pcc_mean' : pcc_m,\n",
    "                       'pcc_std' : pcc_std}, index=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT65H0TVzJlv"
   },
   "source": [
    "Calculates the mean prediction for each test point in a set of experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wOsTcnc2WEyz"
   },
   "outputs": [],
   "source": [
    "\n",
    "  \n",
    "\n",
    "def mean_prediction(file_name, number):\n",
    "\n",
    "  predictions = np.zeros((227, 1))\n",
    "  for x in range(number):\n",
    "    pred = pd.read_csv(file_loc + file_name + '_Predictions_' + str(x) + '.csv')\n",
    "    if pred.shape == (227, 2): # The R code models have a different format in predictions\n",
    "      pred = np.array(pred.iloc[:, 1]).reshape(227,1) \n",
    "    elif pred.shape == (228, 2):\n",
    "      pred = np.array(pred.iloc[1:, 1]).reshape(227,1)\n",
    "    predictions = np.array(pred) + np.array(predictions)\n",
    "  predictions /= number \n",
    " \n",
    "  return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZCN65hdKzziD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EijBuDyJaP-H"
   },
   "outputs": [],
   "source": [
    "experiments_ran = 98\n",
    "#'metablic_expression',\n",
    "names = [ ['sgl_svm', 'sgl', 'Na', 'Early', 'svm'],\n",
    "          ['iRF_svm', 'iRF', 'Na', 'Early', 'svm'],\n",
    "          ['genetic_rf', 'NSGA-II', 'Na', 'Early', 'rf'],\n",
    "         ['genetic_svm', 'NSGA-II', 'Na', 'Early', 'svm'],\n",
    "         ['expression_svm', 'expression', 'Na', 'None', 'svm'], \n",
    "         [ 'bagged_expression_fluxes_rf', 'expression', 'fluxes', 'Late', 'rf'],\n",
    "         ['expression_rf', 'expression', 'Na', 'None', 'rf'],\n",
    "         ['fluxes_rf', 'fluxes', 'Na', 'None', 'rf'],\n",
    "         ['iRF_rf', 'iRF', 'Na', 'Early', 'rf'],\n",
    "         ['sgl_rf', 'sgl', 'Na', 'Early', 'rf'],\n",
    "         ['metabolic_expression_rf', 'metabolic_expression', 'Na', 'None', 'rf'],\n",
    "         ['fluxes_svm', 'fluxes', 'Na', 'None', 'svm'],\n",
    "         ['metabolic_expression_svm', 'metabolic_expression', 'Na', 'None', 'svm'],\n",
    "         ['metabolic_expression', 'metabolic_expression', 'Na', 'None', 'dl'],\n",
    "         ['expression', 'expression', 'Na', 'None', 'dl'],\n",
    "         ['fluxes', 'fluxes', 'Na', 'None', 'dl'],\n",
    "         ['concat_expression_fluxes_svm', 'expression', 'fluxes', 'Early', 'svm'],\n",
    "         ['concate_Flu_GE', 'expression', 'fluxes', 'Early', 'dl'],\n",
    "         ['fluxes', 'fluxes', 'Na', 'None', 'dl'],\n",
    "         ['concate_Flu_GE', 'expression', 'fluxes', 'Early', 'dl'],\n",
    "         ['iRF', 'iRF', 'Na', 'Early', 'dl'],\n",
    "         ['multi_model_metabolic_expression', 'metabolic_expression', 'fluxes', 'Intermediate', 'dl'],\n",
    "         ['multi_model_full_expression', 'expression', 'fluxes', 'Intermediate', 'dl'],\n",
    "         ['SGL', 'SGL', 'Na', 'Early', 'dl'],\n",
    "         ['NSGA-II', 'NSGA-II', 'Na', 'Early', 'dl'],\n",
    "         ['metabolic_expression_bemkl', 'metabolic_expression', 'fluxes', 'Intermediate', 'svm'],\n",
    "         ['expression_bemkl', 'expression', 'fluxes', 'Intermediate', 'svm'],\n",
    "         ['concat_expression_fluxes_rf', 'expression', 'fluxes', 'Early', 'rf'],\n",
    "         ['bagged_metabolic_expression_fluxes_rf_', 'metabolic_expression', 'fluxes', 'Early', 'rf']]\n",
    "\n",
    "\n",
    "\n",
    "results_averaged = pd.DataFrame(columns = ['file_name', 'method_type', 'data_1', 'data_2', 'integration_type', 'mae_mean', 'mae_std', 'mdae_mean', 'mdae_std', 'rmse_mean', 'rmse_std', 'pcc_mean',  'pcc_std'])\n",
    "results_all = pd.DataFrame(columns = ['file_name', 'method_type', 'data_1', 'data_2', 'integration_type', 'mae',\n",
    "                                      'mdae', 'rmse', 'pcc'])\n",
    "\n",
    "predictions_vs_actual = pd.DataFrame()\n",
    "\n",
    "for x in names: \n",
    "  results_averaged = pd.concat([summarise_results(x[0], x[1], x[2], x[3], x[4], experiments_ran), results_averaged], ignore_index=True)\n",
    "  results_all = pd.concat([get_stats_for_each_run(x[0], x[1], x[2], x[3], x[4], experiments_ran), results_all], ignore_index=True)\n",
    "  print(results_averaged)\n",
    "  mean_pred  =  mean_prediction(x[0], experiments_ran)\n",
    "  prediction_minus_target = mean_pred - target\n",
    "  predictions_vs_actual[x[0]] = prediction_minus_target[:, 0] \n",
    "\n",
    "results_all.to_csv(data_loc + 'full_results_singles_100_experiments.csv', index = False)\n",
    "results_averaged.to_csv(data_loc + 'full_results_100_experiments.csv', index = False)\n",
    "predictions_vs_actual.to_csv(data_loc + 'predictions_minus_target.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kLZRhaj8ziP4"
   },
   "source": [
    "# Gathering Error Statistics on the Test Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "njiwmXkOeYyD"
   },
   "outputs": [],
   "source": [
    "# Location of the independent test set \n",
    "independent_target = pd.read_csv(data_loc + '/independent_target.csv', header = None)\n",
    "independent_loc = file_loc + 'Independent/'\n",
    "\n",
    "# Which indicies relate to the double and single knockouts\n",
    "double_knockouts = pd.read_csv(data_loc +  'is_double_gene_knockout.csv', header = None) > 0 \n",
    "double_knockouts_index = np.where(double_knockouts)[0]\n",
    "single_knockouts_index = np.where(-double_knockouts)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RUYUov-Mz3sB"
   },
   "source": [
    "Here we take in a set of results on the independent dataset and produce a table of metrics on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2L1pgmHnz0XA"
   },
   "outputs": [],
   "source": [
    "\n",
    "def summarise_independent(data, algorithm, imputation_type,file_name,  double_only = False, single_only = False, header = True):\n",
    "    \n",
    "    if not header:\n",
    "      pred = pd.read_csv(independent_loc + file_name, header=None)\n",
    "    else:\n",
    "      pred = pd.read_csv(independent_loc + file_name)\n",
    "\n",
    "    if pred.shape[1] == 2: # The R code models have a different format in predictions\n",
    "      pred = pred.iloc[:, 1].to_frame()\n",
    "    if double_only:\n",
    "      pred = pred.iloc[double_knockouts_index]\n",
    "      target = independent_target.iloc[double_knockouts_index, :]\n",
    "      data_gene_type = 'double'\n",
    "    elif single_only:\n",
    "      pred = pred.iloc[single_knockouts_index]\n",
    "      target = independent_target.iloc[single_knockouts_index, :]\n",
    "      data_gene_type = 'single'\n",
    "    else: \n",
    "      target = independent_target\n",
    "      data_gene_type = 'all'\n",
    "\n",
    "    mae = mean_absolute_error(pred, target)\n",
    "    mdae = median_absolute_error(pred, target)\n",
    "    rmse = np.sqrt(mean_squared_error(pred,target))\n",
    "    pred = pred.to_numpy().squeeze()\n",
    "    pcc = scipy.stats.pearsonr(pred, target.squeeze())[0]\n",
    "    return pd.DataFrame({ 'data' : data, \n",
    "                          'algorithm' : algorithm,\n",
    "                         'imputation_method'  : imputation_type,\n",
    "                         'genes' : data_gene_type, \n",
    "                       'mae' : mae,\n",
    "                       'mdae' : mdae,\n",
    "                       'mdae' : mdae,\n",
    "                       'rmse' : rmse,\n",
    "                       'pcc' : pcc}, index=[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L0JFFKeNFZgn"
   },
   "source": [
    "Creating a summary csv to be exported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DGjZ_gpmeyym"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "independent_results = summarise_independent('expression', 'svm', 'lr', 'expression_only_svm_independent_lr.csv')\n",
    "independent_results = pd.concat([summarise_independent('expression', 'svm', 'lr', 'expression_only_svm_independent_lr.csv', double_only=True), independent_results], ignore_index = True )\n",
    "independent_results = pd.concat([summarise_independent('expression', 'svm', 'lr', 'expression_only_svm_independent_lr.csv', single_only = True), independent_results], ignore_index = True )\n",
    "\n",
    "\n",
    "independent_results = pd.concat([summarise_independent('expression', 'svm', 'm', 'expression_only_svm_independent_m.csv'), independent_results], ignore_index = True )\n",
    "independent_results = pd.concat([summarise_independent('expression', 'svm', 'm', 'expression_only_svm_independent_m.csv', double_only=True), independent_results], ignore_index = True )\n",
    "independent_results = pd.concat([summarise_independent('expression', 'svm', 'm', 'expression_only_svm_independent_m.csv', single_only = True), independent_results], ignore_index = True )\n",
    "\n",
    "independent_results = pd.concat([summarise_independent('expression', 'dl', 'lr', 'independent_lr_dl_expression_only', header = False),independent_results], ignore_index = True )\n",
    "independent_results = pd.concat([summarise_independent('expression', 'dl', 'lr', 'independent_lr_dl_expression_only', header = False, double_only=True),independent_results], ignore_index = True )\n",
    "independent_results = pd.concat([summarise_independent('expression', 'dl', 'lr', 'independent_lr_dl_expression_only', header = False, single_only = True),independent_results], ignore_index = True )\n",
    "\n",
    "independent_results = pd.concat([summarise_independent('expression', 'dl', 'm', 'independent_m_dl_expression_only', header = False),independent_results], ignore_index = True )\n",
    "independent_results = pd.concat([summarise_independent('expression', 'dl', 'm', 'independent_m_dl_expression_only', header = False, double_only = True),independent_results], ignore_index = True )\n",
    "independent_results = pd.concat([summarise_independent('expression', 'dl', 'm', 'independent_m_dl_expression_only', header = False, single_only = True),independent_results], ignore_index = True )\n",
    "\n",
    "independent_results = pd.concat([summarise_independent('expression + fluxes', 'dl', 'lr', 'independent_lr_multi_modal_expression_fluxes', header = False), independent_results], ignore_index = True )\n",
    "independent_results = pd.concat([summarise_independent('expression + fluxes', 'dl', 'lr', 'independent_lr_multi_modal_expression_fluxes', header = False, double_only = True), independent_results], ignore_index = True )\n",
    "independent_results = pd.concat([summarise_independent('expression + fluxes', 'dl', 'lr', 'independent_lr_multi_modal_expression_fluxes', header = False, single_only = True), independent_results], ignore_index = True )\n",
    "\n",
    "independent_results = pd.concat([summarise_independent('expression + fluxes', 'dl', 'm', 'independent_m_multi_modal_expression_fluxes', header = False),independent_results],  ignore_index = True )\n",
    "independent_results = pd.concat([summarise_independent('expression + fluxes', 'dl', 'm', 'independent_m_multi_modal_expression_fluxes', header = False, double_only = True),independent_results],  ignore_index = True )\n",
    "independent_results = pd.concat([summarise_independent('expression + fluxes', 'dl', 'm', 'independent_m_multi_modal_expression_fluxes', header = False, single_only = True),independent_results],  ignore_index = True )\n",
    "\n",
    "\n",
    "independent_results.to_csv(data_loc + 'independent_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W-djqm--1I6s"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hVzNbJJC2ZT2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Analyse_Results.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
